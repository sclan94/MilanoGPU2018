{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(7): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    // First stride through global memory and compute\n",
    "    // the maximum for every thread\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx.x is always zero because we use just one block\n",
    "    \n",
    "    float max_value = -999999999.999; // FIX ME!\n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Temporary write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[128];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Synchronize so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "        \n",
    "    // Find the maximum of shared memory\n",
    "    \n",
    "    // Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 64]);\n",
    "    }\n",
    "    \n",
    "    // Since we have here more than one active warp (threadIdx.x > 32), we need to make\n",
    "    // sure all threads have finished before continuing\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Reduce from 64 to 32 elements\n",
    "    if (threadIdx.x < 32) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 32]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 32 to 16 elements\n",
    "    if (threadIdx.x < 16) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 16 to 8 elements\n",
    "    if (threadIdx.x < 8) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 8]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 8 to 4 elements\n",
    "    if (threadIdx.x < 4) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 4]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 4 to 2 elements\n",
    "    if (threadIdx.x < 2) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 2]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 2 to 1 elements\n",
    "    if (threadIdx.x < 1) {\n",
    "    max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 1]);\n",
    "    }\n",
    "    \n",
    "    // Eventually write out to output\n",
    "    if (threadIdx.x == 0) {\n",
    "        output[0] = max_shared[0];\n",
    "    }    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 256\n",
    "a = np.random.random((1, n)).astype(np.float32)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 128\n",
    "b = np.empty((1, num_threads), dtype=np.float32)\n",
    "\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98645794 0.11465081 0.7008942  0.71582574 0.19230945 0.6307102\n",
      "  0.99297667 0.8555335  0.78378296 0.503948   0.5153691  0.9033084\n",
      "  0.93761325 0.92897975 0.77439135 0.7345318  0.49218673 0.7092952\n",
      "  0.4174651  0.79176825 0.9132511  0.8660131  0.54959965 0.7723747\n",
      "  0.3197224  0.8160064  0.71256185 0.5057406  0.488644   0.39682478\n",
      "  0.84822094 0.81397504 0.5303921  0.92334276 0.76257855 0.8678269\n",
      "  0.87212366 0.9366278  0.63259107 0.7359021  0.9261337  0.23706184\n",
      "  0.70179826 0.30871013 0.36185932 0.6378562  0.81562024 0.16962792\n",
      "  0.948172   0.6919221  0.4522929  0.5903881  0.5093269  0.6629031\n",
      "  0.8754085  0.8933242  0.7824821  0.6522927  0.82839596 0.8040152\n",
      "  0.54342407 0.8473027  0.9494782  0.78500223 0.4642936  0.9062707\n",
      "  0.4790082  0.5832815  0.35919693 0.32458237 0.66091955 0.8869996\n",
      "  0.8783214  0.82406616 0.6984912  0.8067058  0.8294999  0.81367797\n",
      "  0.4305992  0.52413195 0.8388642  0.73126245 0.8397078  0.98645794\n",
      "  0.7415238  0.33788818 0.08715762 0.47475618 0.9221223  0.826675\n",
      "  0.6476152  0.86708707 0.41905737 0.702603   0.8904504  0.85167205\n",
      "  0.49835873 0.8282299  0.44967455 0.45019865 0.948333   0.6426917\n",
      "  0.5284732  0.97398764 0.93102    0.33710995 0.83291245 0.5506225\n",
      "  0.86458874 0.89631593 0.21413833 0.39926785 0.89598054 0.63129145\n",
      "  0.48572314 0.9926096  0.6826119  0.67260706 0.70850676 0.868473\n",
      "  0.38281354 0.46966663 0.89789087 0.5499079  0.7842686  0.5752694\n",
      "  0.26701382 0.41066518]]\n",
      "0.99297667\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), np.int32(n), grid=grid_size, block=block_size)\n",
    "\n",
    "#Download data\n",
    "b_g.get(b)\n",
    "\n",
    "#print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
